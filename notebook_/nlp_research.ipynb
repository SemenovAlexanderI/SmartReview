{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5d356d",
   "metadata": {},
   "source": [
    "# NLP Research: Классификация и Суммаризация отзывов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9729c68",
   "metadata": {},
   "source": [
    "## Цель исследования\n",
    "В этом модуле мы проводим эксперименты с методами обработки естественного языка (NLP) для анализа отзывов Amazon. \n",
    "Наша задача - разработать конвейер, который может:\n",
    "1.  Определять тональность текста (Sentiment Analysis).\n",
    "2.  Классифицировать тип обращения (Жалоба/Вопрос/Благодарность).\n",
    "3.  Генерировать краткое содержание (Summarization) длинных отзывов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d190d",
   "metadata": {},
   "source": [
    "## Методология\n",
    "* **EDA:** Анализ распределения длин текстов и баланса классов.\n",
    "* **Heuristics:** Создание Baseline-классификатора на основе ключевых слов.\n",
    "* **Fine-tuning:** Дообучение `DistilBERT` для классификации тональности.\n",
    "* **Seq2Seq:** Использование `Flan-T5` для суммаризации текста с применением стратегии чанкинга (chunking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48192268",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5563900",
   "metadata": {},
   "source": [
    "Здесь все просто, стандартная загрузка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загружаем классический датасет Amazon Polarity\n",
    "ds = load_dataset('amazon_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76007953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# датасет содержит поля 'title', 'content' и 'label' (0 - neg, 1 - pos)(то что надо для классификации)\n",
    "print(\"примеры:\")\n",
    "for i in range(5):\n",
    "    print('---')\n",
    "    print(ds['train'][i])\n",
    "    print(f'label: {ds[\"train\"][i][\"label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcf80f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e15ed8",
   "metadata": {},
   "source": [
    "# Разведочный анализ данных\n",
    "---\n",
    "## 1. Exploratory Data Analysis (EDA)\n",
    "Для удобства анализа преобразуем часть датасета в **Pandas DataFrame**. \n",
    "Нам важно понять распределение длины текстов, так как это влияет на выбор `max_length` при токенизации и стратегии суммаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание небольшого датасета для теста \n",
    "small_train = ds['train'].select(range(2000))\n",
    "small_test = ds['test'].select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f28f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def ds_to_df(dataset, text='content', label_col='label', n = 2000):\n",
    "    \"\"\"Преобразует датасет HuggingFace в DataFrame pandas.\"\"\"\n",
    "    items = dataset.select(range(min(len(dataset), n)))\n",
    "    df = pd.DataFrame({'text':[x[text] for x in items], 'label':[x[label_col] for x in items]})\n",
    "    return df\n",
    "\n",
    "df = ds_to_df(small_train, n=2000)\n",
    "# print(df['label'].value_counts())\n",
    "# df['char_len'] = df['text'].apply(len)\n",
    "\n",
    "# print(f\"\\nChar_len stats: {df['char_len'].describe()}\")\n",
    "\n",
    "# print(\"\\nLong text:\\n\", df.loc[df['char_len'].idxmax(), 'text'][:500])\n",
    "# print(\"\\nShort text:\\n\", df.loc[df['char_len'].idxmin(), 'text'])\n",
    "\n",
    "# 1. Проверка баланса классов\n",
    "print(\"Распределение меток (0=Neg, 1=Pos):\")\n",
    "print(df['label'].value_counts())\n",
    "count_neg , count_pos = df['label'].value_counts().sort_index().values\n",
    "print(f\"Распределение: Neg = {count_neg} ({count_neg/len(df)*100:.2f}%), Pos = {count_pos} ({count_pos/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 2. Анализ длины текстов (в символах)\n",
    "df['char_len'] = df['text'].apply(len)\n",
    "print(f\"\\nСтатистика длины текстов:\\n{df['char_len'].describe()}\")\n",
    "\n",
    "# Посмотрим на выбросы (самый длинный и самый короткий)\n",
    "print(\"\\n--> Longest text sample (first 500 chars):\\n\", df.loc[df['char_len'].idxmax(), 'text'][:500])\n",
    "print(\"\\n--> Shortest text sample:\\n\", df.loc[df['char_len'].idxmin(), 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c7e7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e565e4",
   "metadata": {},
   "source": [
    "# Эвристическая классификация (Rule-based)\n",
    "---\n",
    "## 2. Rule-Based Classification (Baseline)\n",
    "Помимо тональности, нам важно понимать *интент* пользователя (жалоба, вопрос, похвала). \n",
    "В качестве базового решения (Baseline) реализуем классификатор на основе ключевых слов. В продакшене это может служить быстрым пре-фильтром."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdf746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Словари ключевых слов ---\n",
    "# жалоба \n",
    "COMPLAINT_KW = ['not', 'no', 'dirty', 'broken', 'complain', 'problem', 'issue', 'refund', 'late', 'rude', 'terrible']\n",
    "# комплименты\n",
    "PRAISE_KW = ['great', 'excellent', 'clean', 'friendly', 'love', 'amazing', 'perfect', 'good', 'recommend']\n",
    "# вопрос\n",
    "QUESTION_KW = ['how', 'what', 'where', 'when', 'why', 'is there', 'do you', 'could you']\n",
    "# предложение \n",
    "SUGGESTION_KW = ['should', 'could', 'would be', 'suggest', 'recommendation', 'idea']\n",
    "\n",
    "def classify_type_rule(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Простая эвристика: проверяет наличие ключевых слов в тексте.\n",
    "    Приоритет: Жалоба -> Похвала -> Вопрос -> Предложение\n",
    "    \"\"\"\n",
    "    t = text.lower()\n",
    "    for kw in COMPLAINT_KW:\n",
    "        if kw in t:\n",
    "            return 'complaint'\n",
    "    for kw in PRAISE_KW:\n",
    "        if kw in t:\n",
    "            return 'praise'\n",
    "    for kw in QUESTION_KW:\n",
    "        if kw in t:\n",
    "            return 'question'\n",
    "    for kw in SUGGESTION_KW:\n",
    "        if kw in t:\n",
    "            return 'suggestion'\n",
    "    return 'other'\n",
    "\n",
    "# Тестируем на случайных примерах\n",
    "print(\"--- Тест эвристики ---\")\n",
    "sample_text = df['text'].sample(9, random_state=42).tolist()\n",
    "for s in sample_text:\n",
    "    print('-----\\n', s[:100])\n",
    "    print('----> type:', classify_type_rule(s))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6390b",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения (Tokenization)\n",
    "---\n",
    "## 3. Подготовка данных для Fine-Tuning\n",
    "Мы будем дообучать модель `DistilBERT` для классификации тональности. \n",
    "На этом этапе мы:\n",
    "1. Создаем маппинг меток (хотя для binary classification это опционально, здесь мы добавляем `type_id` для демонстрации возможности мультиклассового обучения).\n",
    "2. Токенизируем тексты (превращаем в `input_ids`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b99d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c84ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# создаем DataFrame c колонами: text, sentiment_label(0/1), type_label(0,...,4)\n",
    "df_small = pd.DataFrame({\n",
    "    'text': [x['content'] for x in small_train],\n",
    "    'labels': [x['label'] for x in small_train]\n",
    "}) \n",
    "\n",
    "# добавляем колонку с типом отзыва по эвристике\n",
    "df_small['type'] = df_small['text'].apply(classify_type_rule)\n",
    "\n",
    "type2id = {t:i for i, t in enumerate(df_small['type'].unique())}\n",
    "id2type = {i:t for t,i in type2id.items()}\n",
    "df_small['type_id'] = df_small['type'].map(type2id)\n",
    "\n",
    "# Создаем маппинг категорий в ID\n",
    "print(f'type2id: {type2id}')\n",
    "print(df_small.head(5))\n",
    "\n",
    "# Преобразуем в датасет HuggingFace\n",
    "hf_ds = Dataset.from_pandas((df_small[['text', 'labels', 'type_id']]))\n",
    "print(f'shape hf_df:{hf_ds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9859d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Токенизация ---\n",
    "\n",
    "def tokenizer_fn(batch):\n",
    "    # Truncation=True обрезает тексты длиннее 256 токенов (для скорости обучения)\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "print(\"Токенизация данных...\")\n",
    "hf_ds = hf_ds.map(tokenizer_fn, batched=True)\n",
    "# hf_ds = hf_dds_to_dfs.rename_column('sentiment', 'labels_sentiment')\n",
    "# hf_ds = hf_ds.rename_column('type_id', 'labels_type')\n",
    "# hf_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'labels_type'])\n",
    "\n",
    "# Удаляем текстовые колонки, оставляем только тензоры для PyTorch\n",
    "hf_ds = hf_ds.remove_columns([c for c in hf_ds.column_names if c not in ('input_ids','attention_mask','labels')])\n",
    "hf_ds.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "print(f\"Формат данных для модели: {hf_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b8473",
   "metadata": {},
   "source": [
    "# Обучение модели (Fine-tuning)\n",
    "---\n",
    "## 4. Model Fine-Tuning (DistilBERT)\n",
    "Используем `Trainer API` от Hugging Face для дообучения модели. \n",
    "* **Модель:** DistilBERT (легкая и быстрая).\n",
    "* **Задача:** Binary Classification (Sentiment).\n",
    "* **Метрика:** Accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка окружения\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate \n",
    "print(torch.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164757ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Инициализация модели (num_labels=2 для Pos/Neg)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# Разбиение на train/eval (90/10)\n",
    "full_split = hf_ds.train_test_split(test_size=0.1, seed=42)\n",
    "# Ограничиваем размер для демонстрации\n",
    "train_ds = full_split['train'].select(range(min(4000, len(full_split['train']))))\n",
    "eval_ds = full_split['test'].select(range(min(800, len(full_split['test']))))\n",
    "\n",
    "# Метрика\n",
    "accuracy = evaluate.load('accuracy')\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# Параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './models/sentiment-distilbert',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "print(\"Начало обучения модели...\")\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fb019",
   "metadata": {},
   "source": [
    "# Суммаризация (Chunking strategy)\n",
    "---\n",
    "## 5. Text Summarization (Handling Long Context)\n",
    "Для суммаризации длинных отзывов мы используем модель `Google Flan-T5`.\n",
    "Т.к. отзывы могут превышать контекстное окно модели, реализуем стратегию **Chunking**:\n",
    "1. Разбиваем текст на предложения.\n",
    "2. Группируем предложения в чанки (блоки) фиксированного размера.\n",
    "3. Суммаризируем каждый чанк отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization \n",
    "\n",
    "from transformers import pipeline \n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Загрузка NLTK данных для разбиения на предложения\n",
    "SUM_MODEL = 'google/flan-t5-small'\n",
    "summarization = pipeline('summarization', model=SUM_MODEL)\n",
    "\n",
    "def chunk_sentences(text, max_chars=800):\n",
    "    \"\"\"\n",
    "    Разбивает текст на куски (chunks), не разрывая предложения.\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    curr = []\n",
    "    curr_len = 0\n",
    "    for s in sents:\n",
    "        if curr_len + len(s) > max_chars and curr:\n",
    "            chunks.append(' '.join(curr))\n",
    "            curr = [s]\n",
    "            curr_len = len(s)\n",
    "        else:\n",
    "            curr.append(s)\n",
    "            curr_len += len(s)\n",
    "\n",
    "    if curr:\n",
    "        chunks.append(' '.join(curr))\n",
    "    return chunks\n",
    "\n",
    "# --- Демонстрация работы ---\n",
    "print(\"--- Test Chunking Summary ---\")\n",
    "# Берем длинный текст из датасета\n",
    "sample_long = df_small['text'].iloc[0]\n",
    "chunks = chunk_sentences(sample_long, max_chars=600)\n",
    "print(f'Original Text Length: {len(sample_long)} chars')\n",
    "print(f'Total chunks created: {len(chunks)}')\n",
    "\n",
    "for c in chunks:\n",
    "    print(f'--- chunk ---\\n{c[:300]}')\n",
    "    # max_length - длина саммари, min_length - минимальная длина\n",
    "    out = summarization(c, max_length=50, min_length=10, truncation=True)\n",
    "    print(f\"-> summary: {out[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca93d8",
   "metadata": {},
   "source": [
    "# Финальный пайплайн (Inference)\n",
    "## 6. Unified NLP Pipeline\n",
    "Собираем все компоненты в единую функцию анализа.\n",
    "> **Note:** Для демонстрации инференса мы используем готовую fine-tuned модель `sst-2` (чтобы не зависеть от локального обучения выше), но в реальном кейсе здесь подгружалась бы модель из `./models/sentiment-distilbert`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text, do_summary=True):\n",
    "    \"\"\"\n",
    "    Комплексный анализ текста:\n",
    "    1. Sentiment Analysis (Positive/Negative)\n",
    "    2. Category Classification (Rule-based)\n",
    "    3. Summarization (если текст длинный)\n",
    "    \"\"\"\n",
    "    # загрузка пайплайна для сентимент анализа\n",
    "    from transformers import pipeline as hf_pipeline\n",
    "    # Используем sst-2\n",
    "    sent = hf_pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "    # Обрезаем до 512, так как BERT имеет лимит\n",
    "    sentiment_result = sent(text[:512])[0]['label'].lower()\n",
    "    sentiment_label = 'positive' if 'pos' in sentiment_result else 'negative'\n",
    "\n",
    "    # 2. Категория (эвристика)\n",
    "    t = classify_type_rule(text)\n",
    "\n",
    "    # 3. Суммаризация\n",
    "    summary = None\n",
    "    if do_summary:\n",
    "        chunks = chunk_sentences(text, max_chars=600)\n",
    "        parts = []\n",
    "        for c in chunks:\n",
    "            parts.append(summarization(c, max_length=60, min_length=10, truncation=True)[0]['summary_text'])\n",
    "        # joiu\n",
    "        summary = ' '.join(parts)\n",
    "    return {'->sentiment':sentiment_label, \n",
    "            '->category': t, \n",
    "            '->summary': summary}\n",
    "\n",
    "# print(analyze_text(df_small['text'].iloc[0], do_summary=True))\n",
    "# Тестируем на примере\n",
    "print(\"--- Final Analysis Result ---\")\n",
    "result = analyze_text(df_small['text'].iloc[0], do_summary=True)\n",
    "for k, v in result.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
